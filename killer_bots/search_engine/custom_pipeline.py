import os

from haystack.document_stores import FAISSDocumentStore
from haystack.nodes import PreProcessor, EmbeddingRetriever
from haystack.pipelines import DocumentSearchPipeline
from haystack.utils import convert_files_to_docs
from transformers import AutoTokenizer
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM

from killer_bots.search_engine.utils import change_extentions_to_txt

import re


def clean_wiki_text(text: str) -> str:
    # get rid of multiple new lines
    while "\n\n" in text:
        text = text.replace("\n\n", "\n")

    # remove extremely short lines
    lines = text.split("\n")
    cleaned = []
    for l in lines:
        if len(l) > 30:
            cleaned.append(l)
        elif l[:2] == "==" and l[-2:] == "==":
            cleaned.append(l)
    text = "\n".join(cleaned)

    # add paragraphs (identified by wiki section title which is always in format "==Some Title==")
    text = text.replace("\n==", "\n\n\n==")

    # remove empty paragrahps
    text = re.sub(r"(==.*==\n\n\n)", "", text)

    # remove multiple dashes
    text = re.sub(r"#+ +", "", text)
    text = re.sub(r" +#+", "", text)

    return text


def _get_document_store():
    try:
        os.remove("faiss_document_store.db")
    except:
        pass
    document_store = FAISSDocumentStore(
        sql_url="sqlite:///faiss_document_store.db",
        # embedding_dim=128,
        faiss_index_factory_str="Flat",
        return_embedding=True
    )
    return document_store


def get_documents(doc_dir):
    change_extentions_to_txt(doc_dir)
    docs = convert_files_to_docs(dir_path=doc_dir, clean_func=clean_wiki_text, split_paragraphs=False)

    preprocessor = PreProcessor(
        clean_empty_lines=True,
        clean_whitespace=True,
        clean_header_footer=False,
        split_by="word",
        split_length=100,
        split_respect_sentence_boundary=True,
    )
    docs = preprocessor.process(docs)
    return docs


def get_retriever(document_store):
    retriever = EmbeddingRetriever(
        document_store=document_store,
        embedding_model="sentence-transformers/multi-qa-mpnet-base-dot-v1",
        model_format="sentence_transformers"
    )
    return retriever


def get_file_scores(documents):
    stats = {}
    counter = {}
    for doc in documents:
        if doc.meta["name"] not in stats:
            stats[doc.meta["name"]] = 0
            counter[doc.meta["name"]] = 0
        stats[doc.meta["name"]] += doc.score
        counter[doc.meta["name"]] += 1
    for key in stats:
        stats[key] /= counter[key]
    # sort by score descending order
    stats = {k: v for k, v in sorted(stats.items(), key=lambda item: item[1], reverse=True)}
    return stats


def get_top_docs(documents, stats, top_k=5):
    top_files = list(stats.keys())[:top_k]
    top_docs = []
    for doc in documents:
        if doc.meta["name"] in top_files:
            top_docs.append(doc)
    top_docs = top_docs[:top_k]
    return top_docs


def postprocess_top_docs(top_docs):
    docs_match = {}
    top_docs = sorted(top_docs, key=lambda x: (x.score, x.meta['vector_id']), reverse=True)
    # top_docs = sorted(top_docs, key=lambda x: x.meta['vector_id'], reverse=False)
    # top_docs = sorted(top_docs, key=lambda x: x.meta['name'], reverse=False)
    return top_docs


def postprocess_answer(top_docs):
    joined_text = "\n".join([doc.content for doc in top_docs])
    return joined_text


# it all starts with a question/query
query = "Why does water heated to room temperature feel colder than the air around it?"

# given the question above suppose these documents below were found in some document store
documents = ["when the skin is completely wet. The body continuously loses water by...",
             "at greater pressures. There is an ambiguity, however, as to the meaning of the terms 'heating' and 'cooling'...",
             "are not in a relation of thermal equilibrium, heat will flow from the hotter to the colder, by whatever pathway...",
             "air condition and moving along a line of constant enthalpy toward a state of higher humidity. A simple example ...",
             "Thermal contact conductance In physics, thermal contact conductance is the study of heat conduction between solid ..."]

# concatenate question and support documents into BART input
conditioned_doc = "<P> " + " <P> ".join([d for d in documents])
query_and_docs = "question: {} context: {}".format(query, conditioned_doc)

# below is the abstractive answer generated by the model
[
    "When you heat water to room temperature, it loses heat to the air around it. When you cool it down, it gains heat back from the air, which is why it feels colder than the air surrounding it. It's the same reason why you feel cold when you turn on a fan. The air around you is losing heat, and the water is gaining heat."]


class Pipeline:
    def __init__(self, doc_dir):
        self.document_store = _get_document_store()
        docs = get_documents(doc_dir)
        self.num_docs = len(docs)
        self.document_store.write_documents(docs)
        self.retriever = get_retriever(self.document_store)
        self.document_store.update_embeddings(self.retriever)
        self.document_search_pipeline = DocumentSearchPipeline(self.retriever)
        self.tokenizer = AutoTokenizer.from_pretrained("facebook/opt-30b")
        self.bart_tokenizer = AutoTokenizer.from_pretrained("vblagoje/bart_lfqa")
        self.bart_model = AutoModelForSeq2SeqLM.from_pretrained("vblagoje/bart_lfqa").to(0)

    def get_relevant_docs(self, query):
        documents = self.document_search_pipeline.run(
            query=query,
            params={
                "Retriever": {
                    "top_k": self.num_docs
                }
            }
        )["documents"]
        return documents

    def print_logs(self, query, top_docs, answer):
        print("Query:", query)
        print("Docs:")
        for doc in top_docs:
            print(doc.meta["vector_id"], doc.meta["name"], doc.score)
        num_tokens = len(self.tokenizer(answer).input_ids)
        print(f"Number of tokens: {num_tokens}")

    def __call__(self, query, top_k=5, verbose=False):
        documents = self.get_relevant_docs(query)
        stats = get_file_scores(documents)
        top_docs = get_top_docs(documents, stats, top_k)
        top_docs = postprocess_top_docs(top_docs)
        answer = postprocess_answer(top_docs)
        if verbose:
            conditioned_doc = "<P> " + " <P> ".join([d.content for d in top_docs])
            query_and_docs = "question: {} context: {}".format(query, conditioned_doc)
            model_input = self.bart_tokenizer(query_and_docs, truncation=True, padding=True, return_tensors="pt")

            generated_answers_encoded = self.bart_model.generate(input_ids=model_input["input_ids"].to(0),
                                                                 attention_mask=model_input["attention_mask"].to(0),
                                                                 min_length=200,
                                                                 max_length=256,
                                                                 do_sample=False,
                                                                 early_stopping=True,
                                                                 num_beams=8,
                                                                 temperature=1.0,
                                                                 top_k=None,
                                                                 top_p=None,
                                                                 eos_token_id=self.bart_tokenizer.eos_token_id,
                                                                 no_repeat_ngram_size=3,
                                                                 num_return_sequences=1)
            output = self.bart_tokenizer.batch_decode(generated_answers_encoded, skip_special_tokens=True,
                                                      clean_up_tokenization_spaces=True)
            print("BART answer:", output)
            self.print_logs(query, top_docs, answer)
        return answer


if __name__ == "__main__":
    pipeline = Pipeline("/app/killer-bots/killer_bots/bots/code_guru/database")
    while True:
        query = input("> ")
        print(pipeline(query, verbose=True))
